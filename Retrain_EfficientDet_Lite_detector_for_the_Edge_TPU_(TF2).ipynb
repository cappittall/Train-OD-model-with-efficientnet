{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "license"
      },
      "source": [
        "##### *Copyright 2021 Google LLC*\n",
        "*Licensed under the Apache License, Version 2.0 (the \"License\")*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "id": "rKwqeqWBXANA"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Retrain EfficientDet for the Edge TPU with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "In this tutorial, we'll retrain the EfficientDet-Lite object detection model (derived from [EfficientDet](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html)) using the [TensorFlow Lite Model Maker library](https://www.tensorflow.org/lite/guide/model_maker), and then compile it to run on the [Coral Edge TPU](https://www.coral.ai/products/). All in about 30 minutes.\n",
        "\n",
        "By default, we'll retrain the model using a publicly available dataset of salad photos, teaching the model to recognize a salad and some of the ingredients. But we've also provided code so you can upload your own training dataset in the Pascal VOC XML format.\n",
        "\n",
        "Here's an example of the salad training results:\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/site_and_emails_static_assets/Images/efficientdet-salads.png?\" width=\"400\" hspace=\"0\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHZyMPbMsAns"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_efficientdet_model_maker_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<a href=\"https://github.com/google-coral/tutorials/blob/master/retrain_efficientdet_model_maker_tf2.ipynb\" target=\"_parent\"><img src=\"https://img.shields.io/static/v1?logo=GitHub&label=&color=333333&style=flat&message=View%20on%20GitHub\" alt=\"View in GitHub\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzoNZRp4sVxK"
      },
      "source": [
        "If you want to run the notebook with the salad dataset, you can run the whole thing now by clicking **Runtime > Run all** in the Colab toolbar. But if you want to use your own dataset, then continue down to [Load the training data](#scrollTo=H0XM-oIfhgQ7) and follow the instructions there.\n",
        "\n",
        "**Note:** If using a custom dataset, beware that if your dataset includes more than 20 classes, you'll probably have slower inference speeds compared to if you have fewer classes. This is due to an aspect of the EfficientDet architecture in which a certain layer cannot compile for the Edge TPU when it carries more than 20 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving notices: ...working... done\n",
            "^C\n",
            "\n",
            "CondaError: KeyboardInterrupt\n",
            "\n",
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /opt/conda/envs/new_env\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.9\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2023.08.22-h06a4308_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_0 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.11-h7f8727e_2 \n",
            "  pip                pkgs/main/linux-64::pip-23.2.1-py39h06a4308_0 \n",
            "  python             pkgs/main/linux-64::python-3.9.18-h955ad1f_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-68.0.0-py39h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.41.2-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0 \n",
            "  tzdata             pkgs/main/noarch::tzdata-2023c-h04d1e81_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.38.4-py39h06a4308_0 \n",
            "  xz                 pkgs/main/linux-64::xz-5.4.2-h5eee18b_0 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate new_env\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "usage: conda [-h] [--no-plugins] [-V] COMMAND ...\n",
            "conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'doctor', 'env')\n"
          ]
        }
      ],
      "source": [
        "!conda update -n base -c defaults conda -y\n",
        "!conda create -n new_env python=3.9 -y\n",
        "!conda activate new_env -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libusb-1.0-0 is already the newest version (2:1.0.23-2build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: pycocotools in ./.conda/lib/python3.9/site-packages (2.0.7)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in ./.conda/lib/python3.9/site-packages (from pycocotools) (3.4.3)\n",
            "Requirement already satisfied: numpy in ./.conda/lib/python3.9/site-packages (from pycocotools) (1.25.2)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.conda/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in ./.conda/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in ./.conda/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six in ./.conda/lib/python3.9/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
            "Requirement already satisfied: tqdm in ./.conda/lib/python3.9/site-packages (4.66.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgl1 is already the newest version (1.3.2-1~ubuntu0.20.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install libusb-1.0-0\n",
        "!pip install pycocotools\n",
        "!pip install tqdm\n",
        "!sudo apt-get install libgl1 -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q tflite-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numba\n",
            "  Obtaining dependency information for numba from https://files.pythonhosted.org/packages/a1/d2/e3d9752c53244a5cc7abb0c156e0a13bae3dfd99946f9793872963d946af/numba-0.58.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
            "  Using cached numba-0.58.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba)\n",
            "  Obtaining dependency information for llvmlite<0.42,>=0.41.0dev0 from https://files.pythonhosted.org/packages/0f/7a/f2b4d0ed163197dbc9be054492218b96acef6fffb82bb164e815ef7d781e/llvmlite-0.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached llvmlite-0.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting numpy<1.26,>=1.21 (from numba)\n",
            "  Obtaining dependency information for numpy<1.26,>=1.21 from https://files.pythonhosted.org/packages/69/1f/c95b1108a9972a52d7b1b63ed8ca70466b59b8c1811bd121f1e667cc45d8/numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numba-0.58.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "Using cached llvmlite-0.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
            "Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, llvmlite, numba\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.36.0\n",
            "    Uninstalling llvmlite-0.36.0:\n",
            "      Successfully uninstalled llvmlite-0.36.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.53.0\n",
            "    Uninstalling numba-0.53.0:\n",
            "      Successfully uninstalled numba-0.53.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "roboflow 1.1.4 requires urllib3>=1.26.6, but you have urllib3 1.25.11 which is incompatible.\n",
            "supervision 0.14.0 requires matplotlib<4.0.0,>=3.7.1, but you have matplotlib 3.4.3 which is incompatible.\n",
            "tflite-model-maker 0.4.2 requires numba==0.53, but you have numba 0.58.0 which is incompatible.\n",
            "tflite-model-maker-nightly 0.4.3.dev202307110517 requires numba==0.53, but you have numba 0.58.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llvmlite-0.41.0 numba-0.58.0 numpy-1.25.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall numba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "## Import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "np.object = object\n",
        "np.bool = bool\n",
        "np.complex = complex\n",
        "#np.uint8\n",
        "#np.uint16\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0XM-oIfhgQ7"
      },
      "source": [
        "## Load the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QusnAYjwM3uq"
      },
      "source": [
        "To use the default salad training dataset, just run all the code below as-is.\n",
        "\n",
        "But if you want to train with your own image dataset, follow these steps:\n",
        "\n",
        "1. Be sure your dataset is annotated in Pascal VOC XML (various tools can help create VOC annotations, such as [LabelImg](https://github.com/tzutalin/labelImg#labelimg)). Then create a ZIP file with all your JPG images and XML files (JPG and XML files can all be in one directory or in separate directories).\n",
        "2. Click the **Files** tab in the left panel and just drag-drop your ZIP file there to upload it.\n",
        "3. Use the following drop-down option to set **`use_custom_dataset`** to True.\n",
        "4. If your dataset is already split into separate directories for training, validation, and testing, also set **`dataset_is_split`** to True. (If your dataset is not split, leave it False and we'll split it below.)\n",
        "5. Then skip to [Load your own Pascal VOC dataset](#scrollTo=ZljJ25RAnj5x) and follow the rest of the instructions there.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "wVQ2VQu7MuQa"
      },
      "outputs": [],
      "source": [
        "use_custom_dataset = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "dataset_is_split = False #@param [\"False\", \"True\"] {type:\"raw\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VoZ_Rud2V3"
      },
      "source": [
        "### Load the salads CSV dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "\n",
        "\n",
        "Model Maker requires that we load our dataset using the [`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader) API. So in this case, we'll load it from a CSV file that defines 175 images for training, 25 images for validation, and 25 images for testing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "04ObtdneqvP5"
      },
      "outputs": [],
      "source": [
        "if not use_custom_dataset:\n",
        "  train_data, validation_data, test_data = object_detector.DataLoader.from_csv('gs://cloud-ml-data/img/openimage/csv/salads_ml_use.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W0Njyxoq0gH"
      },
      "source": [
        "If you want to load your own dataset as a CSV file, you can learn more about the format in [Formatting a training data CSV](https://cloud.google.com/vision/automl/object-detection/docs/csv-format). You can load your CSV either from [Cloud Storage](https://cloud.google.com/storage) (as shown above) or from a local path.\n",
        "\n",
        "[`DataLoader`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader) can also load your dataset in other formats, such as from a set of TFRecord files or from a local directory using the Pascal VOC format (shown below for a custom dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZljJ25RAnj5x"
      },
      "source": [
        "### (Optional) Load your own Pascal VOC dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei5BahmPn_wR"
      },
      "source": [
        "To use your custom dataset, you need to modify a few variables here, such as your ZIP filename, your label map, and the path to your images/annotations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: roboflow in ./.conda/lib/python3.9/site-packages (1.1.4)\n",
            "Requirement already satisfied: certifi==2022.12.7 in ./.conda/lib/python3.9/site-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet==4.0.0 in ./.conda/lib/python3.9/site-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in ./.conda/lib/python3.9/site-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in ./.conda/lib/python3.9/site-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/lib/python3.9/site-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in ./.conda/lib/python3.9/site-packages (from roboflow) (3.4.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in ./.conda/lib/python3.9/site-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in ./.conda/lib/python3.9/site-packages (from roboflow) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in ./.conda/lib/python3.9/site-packages (from roboflow) (9.5.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in ./.conda/lib/python3.9/site-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in ./.conda/lib/python3.9/site-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in ./.conda/lib/python3.9/site-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: requests in ./.conda/lib/python3.9/site-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in ./.conda/lib/python3.9/site-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in ./.conda/lib/python3.9/site-packages (from roboflow) (0.14.0)\n",
            "Collecting urllib3>=1.26.6 (from roboflow)\n",
            "  Obtaining dependency information for urllib3>=1.26.6 from https://files.pythonhosted.org/packages/37/dc/399e63f5d1d96bb643404ee830657f4dfcf8503f5ba8fa3c6d465d0c57fe/urllib3-2.0.5-py3-none-any.whl.metadata\n",
            "  Using cached urllib3-2.0.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wget in ./.conda/lib/python3.9/site-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in ./.conda/lib/python3.9/site-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in ./.conda/lib/python3.9/site-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in ./.conda/lib/python3.9/site-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.9/site-packages (from requests->roboflow) (3.2.0)\n",
            "Collecting matplotlib (from roboflow)\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/e0/8b/b62bc50b01bb2d4af96bc0045c39d60209e2701e172789ceace20a0866b2/matplotlib-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached matplotlib-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in ./.conda/lib/python3.9/site-packages (from supervision->roboflow) (4.8.0.76)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in ./.conda/lib/python3.9/site-packages (from supervision->roboflow) (1.11.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/lib/python3.9/site-packages (from matplotlib->roboflow) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/lib/python3.9/site-packages (from matplotlib->roboflow) (4.42.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.9/site-packages (from matplotlib->roboflow) (20.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./.conda/lib/python3.9/site-packages (from matplotlib->roboflow) (6.0.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./.conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->roboflow) (3.16.2)\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in x-ray-detection-3 to voc: 100% [104903674 / 104903674] bytes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Dataset Version Zip to x-ray-detection-3 in voc:: 100%|██████████| 3633/3633 [00:00<00:00, 6704.08it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"0rYY2EVntRmFW7qAAlYa\")\n",
        "project = rf.workspace(\"hakan-etin\").project(\"x-ray-detection-gapqm\")\n",
        "dataset = project.version(3).download(\"voc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv temp/train/*.xml dataset/annotations/\n",
        "!mv temp/train/*.jpg dataset/images/\n",
        "\n",
        "!mv temp/test/*.xml dataset/annotations/\n",
        "!mv temp/test/*.jpg dataset/images/\n",
        "\n",
        "!mv temp/valid/*.xml dataset/annotations/\n",
        "!mv temp/valid/*.jpg dataset/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mz_suhWiqc7A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'YM'}\n"
          ]
        }
      ],
      "source": [
        "if use_custom_dataset:\n",
        "\n",
        "  # The ZIP file you uploaded:\n",
        "  #!unzip dataset.zip\n",
        "\n",
        "  # Your labels map as a dictionary (zero is reserved):\n",
        "  label_map = {1: 'YM'}\n",
        "  print(label_map)\n",
        "\n",
        "  if dataset_is_split:\n",
        "    # If your dataset is already split, specify each path:\n",
        "    train_images_dir = 'dataset/train/images'\n",
        "    train_annotations_dir = 'dataset/train/annotations'\n",
        "    val_images_dir = 'dataset/validation/images'\n",
        "    val_annotations_dir = 'dataset/validation/annotations'\n",
        "    test_images_dir = 'dataset/test/images'\n",
        "    test_annotations_dir = 'dataset/test/annotations'\n",
        "  else:\n",
        "    # If it's NOT split yet, specify the path to all images and annotations\n",
        "    images_in = 'dataset/images'\n",
        "    annotations_in = 'dataset/annotations'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Rh3IWRb0xw"
      },
      "source": [
        "Now you're ready to train the model with your custom dataset. But before you run the notebook, you should also skip to the [Export to TensorFlow Lite](#scrollTo=_yB_XMpqGlLs) section and change the `TFLITE_FILENAME` and `LABLES_FILENAME` for your exported files.\n",
        "\n",
        "Then run the whole notebook by clicking **Runtime > Run all**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "C35meprE8xzf"
      },
      "outputs": [],
      "source": [
        "#@markdown Be sure you run this cell. It's hiding the `split_dataset()` function used in the next code block.\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(images_path, annotations_path, val_split, test_split, out_path):\n",
        "  \"\"\"Splits a directory of sorted images/annotations into training, validation, and test sets.\n",
        "\n",
        "  Args:\n",
        "    images_path: Path to the directory with your images (JPGs).\n",
        "    annotations_path: Path to a directory with your VOC XML annotation files,\n",
        "      with filenames corresponding to image filenames. This may be the same path\n",
        "      used for images_path.\n",
        "    val_split: Fraction of data to reserve for validation (float between 0 and 1).\n",
        "    test_split: Fraction of data to reserve for test (float between 0 and 1).\n",
        "  Returns:\n",
        "    The paths for the split images/annotations (train_dir, val_dir, test_dir)\n",
        "  \"\"\"\n",
        "  _, dirs, _ = next(os.walk(images_path))\n",
        "\n",
        "  train_dir = os.path.join(out_path, 'train')\n",
        "  val_dir = os.path.join(out_path, 'validation')\n",
        "  test_dir = os.path.join(out_path, 'test')\n",
        "\n",
        "  IMAGES_TRAIN_DIR = os.path.join(train_dir, 'images')\n",
        "  IMAGES_VAL_DIR = os.path.join(val_dir, 'images')\n",
        "  IMAGES_TEST_DIR = os.path.join(test_dir, 'images')\n",
        "  os.makedirs(IMAGES_TRAIN_DIR, exist_ok=True)\n",
        "  os.makedirs(IMAGES_VAL_DIR, exist_ok=True)\n",
        "  os.makedirs(IMAGES_TEST_DIR, exist_ok=True)\n",
        "\n",
        "  ANNOT_TRAIN_DIR = os.path.join(train_dir, 'annotations')\n",
        "  ANNOT_VAL_DIR = os.path.join(val_dir, 'annotations')\n",
        "  ANNOT_TEST_DIR = os.path.join(test_dir, 'annotations')\n",
        "  os.makedirs(ANNOT_TRAIN_DIR, exist_ok=True)\n",
        "  os.makedirs(ANNOT_VAL_DIR, exist_ok=True)\n",
        "  os.makedirs(ANNOT_TEST_DIR, exist_ok=True)\n",
        "\n",
        "  # Get all filenames for this dir, filtered by filetype\n",
        "  filenames = os.listdir(os.path.join(images_path))\n",
        "  filenames = [os.path.join(images_path, f) for f in filenames if (f.endswith('.jpg'))]\n",
        "  # Shuffle the files, deterministically\n",
        "  filenames.sort()\n",
        "  random.seed(42)\n",
        "  random.shuffle(filenames)\n",
        "  # Get exact number of images for validation and test; the rest is for training\n",
        "  val_count = int(len(filenames) * val_split)\n",
        "  test_count = int(len(filenames) * test_split)\n",
        "  for i, file in enumerate(filenames):\n",
        "    source_dir, filename = os.path.split(file)\n",
        "    annot_file = os.path.join(annotations_path, filename.replace(\".jpg\", \".xml\"))\n",
        "    if i < val_count:\n",
        "      shutil.copy(file, IMAGES_VAL_DIR)\n",
        "      shutil.copy(annot_file, ANNOT_VAL_DIR)\n",
        "    elif i < val_count + test_count:\n",
        "      shutil.copy(file, IMAGES_TEST_DIR)\n",
        "      shutil.copy(annot_file, ANNOT_TEST_DIR)\n",
        "    else:\n",
        "      shutil.copy(file, IMAGES_TRAIN_DIR)\n",
        "      shutil.copy(annot_file, ANNOT_TRAIN_DIR)\n",
        "  return (train_dir, val_dir, test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KWROlVNA54xZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'YM'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train count: 1090\n",
            "validation count: 362\n",
            "test count: 362\n"
          ]
        }
      ],
      "source": [
        "print(label_map)\n",
        "# We need to instantiate a separate DataLoader for each split dataset\n",
        "if use_custom_dataset:\n",
        "  if dataset_is_split:\n",
        "    train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        train_images_dir, train_annotations_dir, label_map=label_map)\n",
        "    validation_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        val_images_dir, val_annotations_dir, label_map=label_map)\n",
        "    test_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        test_images_dir, test_annotations_dir, label_map=label_map)\n",
        "  else:\n",
        "    train_dir, val_dir, test_dir = split_dataset(images_in, annotations_in,\n",
        "                                                 val_split=0.2, test_split=0.2,\n",
        "                                                 out_path='split-dataset')\n",
        "    train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        os.path.join(train_dir, 'images'),\n",
        "        os.path.join(train_dir, 'annotations'), label_map=label_map)\n",
        "    validation_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        os.path.join(val_dir, 'images'),\n",
        "        os.path.join(val_dir, 'annotations'), label_map=label_map)\n",
        "    test_data = object_detector.DataLoader.from_pascal_voc(\n",
        "        os.path.join(test_dir, 'images'),\n",
        "        os.path.join(test_dir, 'annotations'), label_map=label_map)\n",
        "\n",
        "  print(f'train count: {len(train_data)}')\n",
        "  print(f'validation count: {len(validation_data)}')\n",
        "  print(f'test count: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8clx0KPutCM"
      },
      "source": [
        "## Select the model spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn61LJ9QbOPi"
      },
      "source": [
        "Model Maker supports the EfficientDet-Lite family of object detection models that are compatible with the Edge TPU. (EfficientDet-Lite is derived from [EfficientDet](https://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html), which offers state-of-the-art accuracy in a small model size). There are several model sizes you can choose from:\n",
        "\n",
        "|| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|-|--------------------|-----------|---------------|----------------------|\n",
        "|| EfficientDet-Lite0 | 5.7       | 37.4            | 30.4%               |\n",
        "|| EfficientDet-Lite1 | 7.6       | 56.3            | 34.3%               |\n",
        "|| EfficientDet-Lite2 | 10.2      | 104.6           | 36.0%               |\n",
        "|| EfficientDet-Lite3 | 14.4      | 107.6           | 39.4%               |\n",
        "| <td colspan=4><br><i>* File size of the compiled Edge TPU models. <br/>** Latency measured on a desktop CPU with a Coral USB Accelerator. <br/>*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.</i></td> |\n",
        "\n",
        "Beware that the Lite2 and Lite3 models do not fit onto the Edge TPU's onboard memory, so you'll see even greater latency when using those, due to the cost of fetching data from the host system memory. Maybe this extra latency is okay for your application, but if it's not and you require the precision of the larger models, then you can [pipeline the model across multiple Edge TPUs](https://coral.ai/docs/edgetpu/pipeline/) (more about this when we compile the model below).\n",
        "\n",
        "For this tutorial, we'll use Lite0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SM9gePHw9Jv1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:19:07.464081: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /workspaces/codespaces-jupyter/.conda/lib/python3.9/site-packages/cv2/../../lib64:\n",
            "2023-09-23 11:19:07.464116: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-09-23 11:19:07.464146: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-4fce10): /proc/driver/nvidia/version does not exist\n",
            "2023-09-23 11:19:07.482866: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "spec = object_detector.EfficientDetLite2Spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnCzdzs0-Rbo"
      },
      "source": [
        "The [`EfficientDetLite0Spec`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/EfficientDetLite0Spec) constructor also supports several arguments that specify training options, such as the max number of detections (default is 25 for the TF Lite model) and whether to use Cloud TPUs for training. You can also use the constructor to specify the number of training epochs and the batch size, but you can also specify those in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qjq2UEHCLUi"
      },
      "source": [
        "## Create and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "Now we need to create our model according to the model spec, load our dataset into the model, specify training parameters, and begin training.\n",
        "\n",
        "Using Model Maker, we accomplished all of that with [`create()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/create):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:19:30.765420: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - ETA: 0s - det_loss: 1.9201 - cls_loss: 0.9425 - box_loss: 0.0196 - reg_l2_loss: 0.0762 - loss: 1.9963 - learning_rate: 0.0102 - gradient_norm: 6.9688"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:27:57.140113: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 550s 5s/step - det_loss: 1.9150 - cls_loss: 0.9399 - box_loss: 0.0195 - reg_l2_loss: 0.0762 - loss: 1.9912 - learning_rate: 0.0102 - gradient_norm: 6.9838 - val_det_loss: 3.3107 - val_cls_loss: 2.0069 - val_box_loss: 0.0261 - val_reg_l2_loss: 0.0769 - val_loss: 3.3876\n",
            "Epoch 2/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 1.3352 - cls_loss: 0.6282 - box_loss: 0.0141 - reg_l2_loss: 0.0777 - loss: 1.4129 - learning_rate: 0.0124 - gradient_norm: 6.3458"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:36:23.277703: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 504s 5s/step - det_loss: 1.3335 - cls_loss: 0.6275 - box_loss: 0.0141 - reg_l2_loss: 0.0777 - loss: 1.4112 - learning_rate: 0.0124 - gradient_norm: 6.3278 - val_det_loss: 1.6235 - val_cls_loss: 0.8308 - val_box_loss: 0.0159 - val_reg_l2_loss: 0.0783 - val_loss: 1.7018\n",
            "Epoch 3/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 1.1905 - cls_loss: 0.5786 - box_loss: 0.0122 - reg_l2_loss: 0.0787 - loss: 1.2692 - learning_rate: 0.0123 - gradient_norm: 5.0327"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:44:39.223723: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 487s 4s/step - det_loss: 1.1891 - cls_loss: 0.5775 - box_loss: 0.0122 - reg_l2_loss: 0.0787 - loss: 1.2678 - learning_rate: 0.0123 - gradient_norm: 5.0227 - val_det_loss: 1.3748 - val_cls_loss: 0.6249 - val_box_loss: 0.0150 - val_reg_l2_loss: 0.0790 - val_loss: 1.4539\n",
            "Epoch 4/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 1.1085 - cls_loss: 0.5167 - box_loss: 0.0118 - reg_l2_loss: 0.0793 - loss: 1.1878 - learning_rate: 0.0121 - gradient_norm: 4.4039"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 11:52:46.931977: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 488s 4s/step - det_loss: 1.1068 - cls_loss: 0.5159 - box_loss: 0.0118 - reg_l2_loss: 0.0793 - loss: 1.1861 - learning_rate: 0.0121 - gradient_norm: 4.3935 - val_det_loss: 1.2560 - val_cls_loss: 0.6633 - val_box_loss: 0.0119 - val_reg_l2_loss: 0.0796 - val_loss: 1.3356\n",
            "Epoch 5/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.9944 - cls_loss: 0.4709 - box_loss: 0.0105 - reg_l2_loss: 0.0797 - loss: 1.0741 - learning_rate: 0.0118 - gradient_norm: 4.0343"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:00:54.488978: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "2023-09-23 12:01:27.444521: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 525s 5s/step - det_loss: 0.9971 - cls_loss: 0.4712 - box_loss: 0.0105 - reg_l2_loss: 0.0797 - loss: 1.0769 - learning_rate: 0.0118 - gradient_norm: 4.0417 - val_det_loss: 1.4575 - val_cls_loss: 0.6627 - val_box_loss: 0.0159 - val_reg_l2_loss: 0.0799 - val_loss: 1.5374\n",
            "Epoch 6/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.9786 - cls_loss: 0.4786 - box_loss: 0.0100 - reg_l2_loss: 0.0801 - loss: 1.0587 - learning_rate: 0.0114 - gradient_norm: 4.0098"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:09:39.675247: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 488s 4s/step - det_loss: 0.9765 - cls_loss: 0.4786 - box_loss: 0.0100 - reg_l2_loss: 0.0801 - loss: 1.0566 - learning_rate: 0.0114 - gradient_norm: 3.9992 - val_det_loss: 1.2888 - val_cls_loss: 0.6981 - val_box_loss: 0.0118 - val_reg_l2_loss: 0.0803 - val_loss: 1.3691\n",
            "Epoch 7/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8867 - cls_loss: 0.4203 - box_loss: 0.0093 - reg_l2_loss: 0.0804 - loss: 0.9672 - learning_rate: 0.0110 - gradient_norm: 3.6058"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:17:46.993800: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 487s 4s/step - det_loss: 0.8847 - cls_loss: 0.4200 - box_loss: 0.0093 - reg_l2_loss: 0.0804 - loss: 0.9651 - learning_rate: 0.0110 - gradient_norm: 3.6120 - val_det_loss: 1.1212 - val_cls_loss: 0.5662 - val_box_loss: 0.0111 - val_reg_l2_loss: 0.0805 - val_loss: 1.2017\n",
            "Epoch 8/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8991 - cls_loss: 0.4479 - box_loss: 0.0090 - reg_l2_loss: 0.0807 - loss: 0.9798 - learning_rate: 0.0105 - gradient_norm: 4.1283"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:25:53.683746: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 496s 5s/step - det_loss: 0.8967 - cls_loss: 0.4466 - box_loss: 0.0090 - reg_l2_loss: 0.0807 - loss: 0.9774 - learning_rate: 0.0105 - gradient_norm: 4.1122 - val_det_loss: 1.2170 - val_cls_loss: 0.6175 - val_box_loss: 0.0120 - val_reg_l2_loss: 0.0809 - val_loss: 1.2978\n",
            "Epoch 9/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8807 - cls_loss: 0.4204 - box_loss: 0.0092 - reg_l2_loss: 0.0810 - loss: 0.9617 - learning_rate: 0.0100 - gradient_norm: 3.7871"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:34:08.595908: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 495s 5s/step - det_loss: 0.8826 - cls_loss: 0.4210 - box_loss: 0.0092 - reg_l2_loss: 0.0810 - loss: 0.9636 - learning_rate: 0.0100 - gradient_norm: 3.7875 - val_det_loss: 1.1606 - val_cls_loss: 0.6238 - val_box_loss: 0.0107 - val_reg_l2_loss: 0.0811 - val_loss: 1.2417\n",
            "Epoch 10/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8425 - cls_loss: 0.4075 - box_loss: 0.0087 - reg_l2_loss: 0.0812 - loss: 0.9237 - learning_rate: 0.0095 - gradient_norm: 3.7003"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:42:19.768405: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "2023-09-23 12:42:53.225497: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 516s 5s/step - det_loss: 0.8405 - cls_loss: 0.4070 - box_loss: 0.0087 - reg_l2_loss: 0.0812 - loss: 0.9217 - learning_rate: 0.0095 - gradient_norm: 3.6891 - val_det_loss: 1.1294 - val_cls_loss: 0.5232 - val_box_loss: 0.0121 - val_reg_l2_loss: 0.0813 - val_loss: 1.2107\n",
            "Epoch 11/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8188 - cls_loss: 0.3886 - box_loss: 0.0086 - reg_l2_loss: 0.0814 - loss: 0.9002 - learning_rate: 0.0089 - gradient_norm: 3.6304"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:51:14.245832: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 509s 5s/step - det_loss: 0.8168 - cls_loss: 0.3875 - box_loss: 0.0086 - reg_l2_loss: 0.0814 - loss: 0.8982 - learning_rate: 0.0089 - gradient_norm: 3.6190 - val_det_loss: 1.4773 - val_cls_loss: 0.6950 - val_box_loss: 0.0156 - val_reg_l2_loss: 0.0815 - val_loss: 1.5588\n",
            "Epoch 12/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.8154 - cls_loss: 0.3995 - box_loss: 0.0083 - reg_l2_loss: 0.0816 - loss: 0.8970 - learning_rate: 0.0082 - gradient_norm: 3.6149"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 12:59:45.054406: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 510s 5s/step - det_loss: 0.8130 - cls_loss: 0.3981 - box_loss: 0.0083 - reg_l2_loss: 0.0816 - loss: 0.8946 - learning_rate: 0.0082 - gradient_norm: 3.5992 - val_det_loss: 1.0943 - val_cls_loss: 0.6607 - val_box_loss: 0.0087 - val_reg_l2_loss: 0.0816 - val_loss: 1.1760\n",
            "Epoch 13/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.7850 - cls_loss: 0.3849 - box_loss: 0.0080 - reg_l2_loss: 0.0817 - loss: 0.8667 - learning_rate: 0.0076 - gradient_norm: 3.4513"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:08:15.467215: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 502s 5s/step - det_loss: 0.7843 - cls_loss: 0.3847 - box_loss: 0.0080 - reg_l2_loss: 0.0817 - loss: 0.8660 - learning_rate: 0.0076 - gradient_norm: 3.4482 - val_det_loss: 1.5408 - val_cls_loss: 0.8827 - val_box_loss: 0.0132 - val_reg_l2_loss: 0.0817 - val_loss: 1.6225\n",
            "Epoch 14/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.7566 - cls_loss: 0.3593 - box_loss: 0.0079 - reg_l2_loss: 0.0817 - loss: 0.8383 - learning_rate: 0.0069 - gradient_norm: 3.4500"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:16:36.743343: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 501s 5s/step - det_loss: 0.7597 - cls_loss: 0.3600 - box_loss: 0.0080 - reg_l2_loss: 0.0817 - loss: 0.8415 - learning_rate: 0.0069 - gradient_norm: 3.4510 - val_det_loss: 1.1416 - val_cls_loss: 0.5902 - val_box_loss: 0.0110 - val_reg_l2_loss: 0.0818 - val_loss: 1.2234\n",
            "Epoch 15/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.7596 - cls_loss: 0.3604 - box_loss: 0.0080 - reg_l2_loss: 0.0818 - loss: 0.8415 - learning_rate: 0.0063 - gradient_norm: 3.4849"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:24:57.881037: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "2023-09-23 13:25:31.604423: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 536s 5s/step - det_loss: 0.7628 - cls_loss: 0.3608 - box_loss: 0.0080 - reg_l2_loss: 0.0818 - loss: 0.8446 - learning_rate: 0.0063 - gradient_norm: 3.4872 - val_det_loss: 1.1186 - val_cls_loss: 0.5490 - val_box_loss: 0.0114 - val_reg_l2_loss: 0.0818 - val_loss: 1.2005\n",
            "Epoch 16/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.7224 - cls_loss: 0.3518 - box_loss: 0.0074 - reg_l2_loss: 0.0819 - loss: 0.8043 - learning_rate: 0.0056 - gradient_norm: 3.4753"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:33:55.393996: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 511s 5s/step - det_loss: 0.7271 - cls_loss: 0.3547 - box_loss: 0.0074 - reg_l2_loss: 0.0819 - loss: 0.8089 - learning_rate: 0.0056 - gradient_norm: 3.5029 - val_det_loss: 1.0459 - val_cls_loss: 0.5281 - val_box_loss: 0.0104 - val_reg_l2_loss: 0.0819 - val_loss: 1.1278\n",
            "Epoch 17/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.6714 - cls_loss: 0.3311 - box_loss: 0.0068 - reg_l2_loss: 0.0819 - loss: 0.7533 - learning_rate: 0.0049 - gradient_norm: 3.3234"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:42:26.358810: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 502s 5s/step - det_loss: 0.6698 - cls_loss: 0.3302 - box_loss: 0.0068 - reg_l2_loss: 0.0819 - loss: 0.7517 - learning_rate: 0.0049 - gradient_norm: 3.3130 - val_det_loss: 1.0038 - val_cls_loss: 0.5799 - val_box_loss: 0.0085 - val_reg_l2_loss: 0.0819 - val_loss: 1.0857\n",
            "Epoch 18/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.6653 - cls_loss: 0.3209 - box_loss: 0.0069 - reg_l2_loss: 0.0819 - loss: 0.7472 - learning_rate: 0.0043 - gradient_norm: 3.2122"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:50:47.912777: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 511s 5s/step - det_loss: 0.6656 - cls_loss: 0.3206 - box_loss: 0.0069 - reg_l2_loss: 0.0819 - loss: 0.7475 - learning_rate: 0.0043 - gradient_norm: 3.2052 - val_det_loss: 0.8209 - val_cls_loss: 0.4894 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0819 - val_loss: 0.9029\n",
            "Epoch 19/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.6835 - cls_loss: 0.3358 - box_loss: 0.0070 - reg_l2_loss: 0.0819 - loss: 0.7654 - learning_rate: 0.0036 - gradient_norm: 3.3242"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 13:59:03.310784: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 495s 5s/step - det_loss: 0.6830 - cls_loss: 0.3351 - box_loss: 0.0070 - reg_l2_loss: 0.0819 - loss: 0.7650 - learning_rate: 0.0036 - gradient_norm: 3.3112 - val_det_loss: 0.7410 - val_cls_loss: 0.4377 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0819 - val_loss: 0.8229\n",
            "Epoch 20/30\n",
            "109/109 [==============================] - ETA: 0s - det_loss: 0.6474 - cls_loss: 0.3185 - box_loss: 0.0066 - reg_l2_loss: 0.0819 - loss: 0.7293 - learning_rate: 0.0030 - gradient_norm: 3.3772"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-23 14:07:12.939673: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "2023-09-23 14:07:45.545671: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/109 [==============================] - 514s 5s/step - det_loss: 0.6470 - cls_loss: 0.3188 - box_loss: 0.0066 - reg_l2_loss: 0.0819 - loss: 0.7289 - learning_rate: 0.0030 - gradient_norm: 3.3968 - val_det_loss: 0.8632 - val_cls_loss: 0.4501 - val_box_loss: 0.0083 - val_reg_l2_loss: 0.0819 - val_loss: 0.9452\n",
            "Epoch 21/30\n",
            " 91/109 [========================>.....] - ETA: 1:14 - det_loss: 0.6311 - cls_loss: 0.3188 - box_loss: 0.0062 - reg_l2_loss: 0.0819 - loss: 0.7130 - learning_rate: 0.0025 - gradient_norm: 3.2848"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data=train_data,\n",
        "                               model_spec=spec,\n",
        "                               validation_data=validation_data,\n",
        "                               epochs=30,\n",
        "                               batch_size=10,\n",
        "                               train_whole_model=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n5-o3vvGfnJ"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "Now we'll use the test dataset to evaluate how well the model performs with data it has never seen before.\n",
        "\n",
        "The [`evaluate()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#evaluate) method provides output in the style of [COCO evaluation metrics](https://cocodataset.org/#detection-eval):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-22 21:22:42.921593: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 209715200 exceeds 10% of free system memory.\n",
            "2023-09-22 21:22:43.009361: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 209715200 exceeds 10% of free system memory.\n",
            "2023-09-22 21:22:43.095692: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 209715200 exceeds 10% of free system memory.\n",
            "2023-09-22 21:22:43.155758: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 629145600 exceeds 10% of free system memory.\n",
            "2023-09-22 21:22:43.351998: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 235929600 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 18s 2s/step\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.19150518,\n",
              " 'AP50': 0.5571936,\n",
              " 'AP75': 0.080263086,\n",
              " 'APs': 0.15670604,\n",
              " 'APm': 0.40038162,\n",
              " 'APl': 0.6990099,\n",
              " 'ARmax1': 0.18688525,\n",
              " 'ARmax10': 0.3138599,\n",
              " 'ARmax100': 0.3453055,\n",
              " 'ARs': 0.30731708,\n",
              " 'ARm': 0.5648936,\n",
              " 'ARl': 0.73333335,\n",
              " 'AP_/YM': 0.19150518}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEon9xd2BDS_"
      },
      "source": [
        "Because the default batch size for [EfficientDetLite models](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/EfficientDetSpec) is 64, this needs only 1 step to go through all 25 images in the salad test set. You can also specify the `batch_size` argument when you call [`evaluate()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#evaluate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yB_XMpqGlLs"
      },
      "source": [
        "## Export to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "Next, we'll export the model to the TensorFlow Lite format. By default, the [`export()`](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#export) method performs [full integer post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization), which is exactly what we need for compatibility with the Edge TPU. (Model Maker uses the same dataset we gave to our model spec as a representative dataset, which is required for full-int quantization.)\n",
        "\n",
        "We just need to specify the export directory and format. By default, it exports to TF Lite, but we also want a labels file, so we declare both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Cu9cxX5Qu-e"
      },
      "outputs": [],
      "source": [
        "TFLITE_FILENAME = 'x-ray-effi0-v1.tflite'\n",
        "LABELS_FILENAME = 'labels.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKd6qk7TbxYO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-12 03:08:38.248877: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "2023-09-12 03:09:02.583575: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n",
            "2023-09-12 03:09:09.451483: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
            "2023-09-12 03:09:09.451524: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
            "2023-09-12 03:09:09.469197: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpy20nee_q\n",
            "2023-09-12 03:09:09.582631: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
            "2023-09-12 03:09:09.582684: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpy20nee_q\n",
            "2023-09-12 03:09:09.937275: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
            "2023-09-12 03:09:11.801846: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpy20nee_q\n",
            "2023-09-12 03:09:12.644636: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 3175443 microseconds.\n",
            "2023-09-12 03:09:14.421724: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-09-12 03:09:16.824664: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 6.066 G  ops, equivalently 3.033 G  MACs\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated count of arithmetic ops: 6.066 G  ops, equivalently 3.033 G  MACs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
            "2023-09-12 03:13:59.572554: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 6.066 G  ops, equivalently 3.033 G  MACs\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated count of arithmetic ops: 6.066 G  ops, equivalently 3.033 G  MACs\n"
          ]
        }
      ],
      "source": [
        "model.export(export_dir='.', tflite_filename=TFLITE_FILENAME, label_filename=LABELS_FILENAME,\n",
        "             export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94hZ-exOCRB"
      },
      "source": [
        "### Evaluate the TF Lite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQpahAIBqBPp"
      },
      "source": [
        "Exporting the model to TensorFlow Lite can affect the model accuracy, due to the reduced numerical precision from quantization and because the original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TF Lite model uses global NMS, which is faster but less accurate.\n",
        "\n",
        "Therefore you should always evaluate the exported TF Lite model and be sure it still meets your requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RS3Ell_lqH4e"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Could not open 'x-ray-effi0-v1.tflite'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_(TF2).ipynb Cell 45\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X62sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mevaluate_tflite(TFLITE_FILENAME, test_data)\n",
            "File \u001b[0;32m/workspaces/codespaces-jupyter/.conda/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py:155\u001b[0m, in \u001b[0;36mObjectDetector.evaluate_tflite\u001b[0;34m(self, tflite_filepath, data)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Evaluate the TFLite model.\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m ds \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgen_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_spec, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_spec\u001b[39m.\u001b[39;49mevaluate_tflite(tflite_filepath, ds, \u001b[39mlen\u001b[39;49m(data),\n\u001b[1;32m    156\u001b[0m                                        data\u001b[39m.\u001b[39;49mannotations_json_file)\n",
            "File \u001b[0;32m/workspaces/codespaces-jupyter/.conda/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:373\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.evaluate_tflite\u001b[0;34m(self, tflite_filepath, dataset, steps, json_file)\u001b[0m\n\u001b[1;32m    370\u001b[0m evaluator, label_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_evaluator_and_label_map(json_file)\n\u001b[1;32m    371\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mtake(steps)\n\u001b[0;32m--> 373\u001b[0m lite_runner \u001b[39m=\u001b[39m eval_tflite\u001b[39m.\u001b[39;49mLiteRunner(tflite_filepath, only_network\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    374\u001b[0m progbar \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mProgbar(steps)\n\u001b[1;32m    375\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m    376\u001b[0m   \u001b[39m# Get the output result after post-processing NMS op.\u001b[39;00m\n",
            "File \u001b[0;32m/workspaces/codespaces-jupyter/.conda/lib/python3.9/site-packages/tensorflow_examples/lite/model_maker/third_party/efficientdet/keras/eval_tflite.py:67\u001b[0m, in \u001b[0;36mLiteRunner.__init__\u001b[0;34m(self, tflite_model_path, only_network)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, tflite_model_path, only_network\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Initializes Lite runner with tflite model file.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m      NMS op.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpreter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlite\u001b[39m.\u001b[39;49mInterpreter(tflite_model_path)\n\u001b[1;32m     68\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpreter\u001b[39m.\u001b[39mallocate_tensors()\n\u001b[1;32m     69\u001b[0m   \u001b[39m# Get input and output tensors.\u001b[39;00m\n",
            "File \u001b[0;32m/workspaces/codespaces-jupyter/.conda/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:456\u001b[0m, in \u001b[0;36mInterpreter.__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    449\u001b[0m custom_op_registerers_by_name \u001b[39m=\u001b[39m [\n\u001b[1;32m    450\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    451\u001b[0m ]\n\u001b[1;32m    452\u001b[0m custom_op_registerers_by_func \u001b[39m=\u001b[39m [\n\u001b[1;32m    453\u001b[0m     x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_op_registerers \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    454\u001b[0m ]\n\u001b[1;32m    455\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 456\u001b[0m     _interpreter_wrapper\u001b[39m.\u001b[39;49mCreateWrapperFromFile(\n\u001b[1;32m    457\u001b[0m         model_path, op_resolver_id, custom_op_registerers_by_name,\n\u001b[1;32m    458\u001b[0m         custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[1;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter:\n\u001b[1;32m    460\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFailed to open \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model_path))\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open 'x-ray-effi0-v1.tflite'."
          ]
        }
      ],
      "source": [
        "model.evaluate_tflite(TFLITE_FILENAME, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph88z7PdOeX7"
      },
      "source": [
        "### Try the TFLite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me6_RwPZqNhX"
      },
      "source": [
        "Just to be sure of things, let's run the model ourselves with an image from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eag7jTOASGFW"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'use_custom_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_(TF2).ipynb Cell 46\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# If you're using a custom dataset, we take a random image from the test set:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m use_custom_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   images_path \u001b[39m=\u001b[39m test_images_dir \u001b[39mif\u001b[39;00m dataset_is_split \u001b[39melse\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(test_dir, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bpotential-journey-q7476jxwr7ph9x99/workspaces/codespaces-jupyter/Retrain_EfficientDet_Lite_detector_for_the_Edge_TPU_%28TF2%29.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   filenames \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(images_path))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'use_custom_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# If you're using a custom dataset, we take a random image from the test set:\n",
        "if use_custom_dataset:\n",
        "  images_path = test_images_dir if dataset_is_split else os.path.join(test_dir, \"images\")\n",
        "  filenames = os.listdir(os.path.join(images_path))\n",
        "  random_index = random.randint(0,len(filenames)-1)\n",
        "  INPUT_IMAGE = os.path.join(images_path, filenames[random_index])\n",
        "else:\n",
        "  # Download a test salad image\n",
        "  INPUT_IMAGE = 'salad-test.jpg'\n",
        "  DOWNLOAD_URL = \"https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg\"\n",
        "  !wget -q -O $INPUT_IMAGE $DOWNLOAD_URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBecI78ZaxsO"
      },
      "source": [
        "To simplify our code, we'll use the [PyCoral API](https://coral.ai/docs/reference/py/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmgtGBqua1N3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://google-coral.github.io/py-repo/\n",
            "Requirement already satisfied: pycoral in ./.conda/lib/python3.9/site-packages (2.0.0)\n",
            "Requirement already satisfied: Pillow>=4.0.0 in ./.conda/lib/python3.9/site-packages (from pycoral) (9.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in ./.conda/lib/python3.9/site-packages (from pycoral) (1.23.4)\n",
            "Requirement already satisfied: tflite-runtime==2.5.0.post1 in ./.conda/lib/python3.9/site-packages (from pycoral) (2.5.0.post1)\n"
          ]
        }
      ],
      "source": [
        "! python3 -m pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkXtipXKqXp4"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "\n",
        "import tflite_runtime.interpreter as tflite\n",
        "from pycoral.adapters import common\n",
        "from pycoral.adapters import detect\n",
        "from pycoral.utils.dataset import read_label_file\n",
        "\n",
        "def draw_objects(draw, objs, scale_factor, labels):\n",
        "  \"\"\"Draws the bounding box and label for each object.\"\"\"\n",
        "  COLORS = np.random.randint(0, 255, size=(len(labels), 3), dtype=np.uint8)\n",
        "  for obj in objs:\n",
        "    bbox = obj.bbox\n",
        "    color = tuple(int(c) for c in COLORS[obj.id])\n",
        "    draw.rectangle([(bbox.xmin * scale_factor, bbox.ymin * scale_factor),\n",
        "                    (bbox.xmax * scale_factor, bbox.ymax * scale_factor)],\n",
        "                   outline=color, width=3)\n",
        "    font = ImageFont.truetype(\"LiberationSans-Regular.ttf\", size=15)\n",
        "    draw.text((bbox.xmin * scale_factor + 4, bbox.ymin * scale_factor + 4),\n",
        "              '%s\\n%.2f' % (labels.get(obj.id, obj.id), obj.score),\n",
        "              fill=color, font=font)\n",
        "\n",
        "# Load the TF Lite model\n",
        "labels = read_label_file(LABELS_FILENAME)\n",
        "interpreter = tflite.Interpreter(TFLITE_FILENAME)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Resize the image for input\n",
        "image = Image.open(INPUT_IMAGE)\n",
        "_, scale = common.set_resized_input(\n",
        "    interpreter, image.size, lambda size: image.resize(size, Image.ANTIALIAS))\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "objs = detect.get_objects(interpreter, score_threshold=0.4, image_scale=scale)\n",
        "\n",
        "# Resize again to a reasonable size for display\n",
        "display_width = 500\n",
        "scale_factor = display_width / image.width\n",
        "height_ratio = image.height / image.width\n",
        "image = image.resize((display_width, int(display_width * height_ratio)))\n",
        "draw_objects(ImageDraw.Draw(image), objs, scale_factor, labels)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxgWQyYOqZha"
      },
      "source": [
        "## Compile for the Edge TPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0QLiwCj9Pw6"
      },
      "source": [
        "First we need to download the Edge TPU Compiler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy3QIn_YqaRP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2659  100  2659    0     0  34986      0 --:--:-- --:--:-- --:--:-- 34986\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Get:1 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease [3611 B]\n",
            "Get:2 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6332 B]\n",
            "Get:3 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal/main amd64 Packages [226 kB]\n",
            "Hit:4 https://dl.yarnpkg.com/debian stable InRelease                           \n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease               \n",
            "Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:7 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease       \n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
            "Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3028 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1405 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3515 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2773 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1103 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2922 kB]\n",
            "Hit:12 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease           \n",
            "Fetched 15.3 MB in 2s (9006 kB/s)               \n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "edgetpu-compiler is already the newest version (16.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWewhqFqeL_"
      },
      "source": [
        "Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory.\n",
        "\n",
        "The Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n",
        "\n",
        "One way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n",
        "\n",
        "The following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n",
        "\n",
        "| Model architecture | Minimum TPUs | Recommended TPUs\n",
        "|--------------------|-------|-------|\n",
        "| EfficientDet-Lite0 | 1     | 1     |\n",
        "| EfficientDet-Lite1 | 1     | 1     |\n",
        "| EfficientDet-Lite2 | 1     | 2     |\n",
        "| EfficientDet-Lite3 | 2     | 2     |\n",
        "| EfficientDet-Lite4 | 2     | 3     |\n",
        "\n",
        "If you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZdonJGCqieU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edge TPU Compiler version 16.0.384591198\n",
            "Searching for valid delegate with step 1\n",
            "Try to compile segment with 357 ops\n",
            "Started a compilation timeout timer of 180 seconds.\n",
            "\n",
            "Model compiled successfully in 9782 ms.\n",
            "\n",
            "Input model: data-matrix-v0.tflite\n",
            "Input size: 7.05MiB\n",
            "Output model: data-matrix-v0_edgetpu.tflite\n",
            "Output size: 9.74MiB\n",
            "On-chip memory used for caching model parameters: 7.14MiB\n",
            "On-chip memory remaining for caching model parameters: 1.25KiB\n",
            "Off-chip memory used for streaming uncached model parameters: 228.88KiB\n",
            "Number of Edge TPU subgraphs: 1\n",
            "Total number of operations: 357\n",
            "Operation log: data-matrix-v0_edgetpu.log\n",
            "\n",
            "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
            "Number of operations that will run on Edge TPU: 354\n",
            "Number of operations that will run on CPU: 3\n",
            "See the operation log file for individual operation details.\n",
            "Compilation child process completed within timeout period.\n",
            "Compilation succeeded! \n"
          ]
        }
      ],
      "source": [
        "NUMBER_OF_TPUS =  1\n",
        "\n",
        "!edgetpu_compiler $TFLITE_FILENAME -d --num_segments=$NUMBER_OF_TPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2CjkduY02DF"
      },
      "source": [
        "**Beware when using multiple segments:** The Edge TPU Comiler divides the model such that all segments have roughly equal amounts of parameter data, but that does not mean all segments have the same latency. Especially when dividing an SSD model such as EfficientDet, this results in a latency-imbalance between segments, because SSD models have a large post-processing op that actually executes on the CPU, not on the Edge TPU. So although segmenting your model this way is better than running the whole model on just one Edge TPU, we recommend that you segment the EfficientDet-Lite model using our [profiling-based partitioner tool](https://github.com/google-coral/libcoral/tree/master/coral/tools/partitioner#profiling-based-partitioner-for-the-edge-tpu-compiler), which measures each segment's latency on the Edge TPU and then iteratively adjusts the segmentation sizes to provide balanced latency between all segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyBBvyqx0XRn"
      },
      "source": [
        "## Download the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M43URVgg0ZcB"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      3\u001b[0m files\u001b[39m.\u001b[39mdownload(TFLITE_FILENAME)\n\u001b[1;32m      4\u001b[0m files\u001b[39m.\u001b[39mdownload(TFLITE_FILENAME\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.tflite\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_edgetpu.tflite\u001b[39m\u001b[39m'\u001b[39m))\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(TFLITE_FILENAME)\n",
        "files.download(TFLITE_FILENAME.replace('.tflite', '_edgetpu.tflite'))\n",
        "files.download(LABELS_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fYwl4Rt8myF"
      },
      "source": [
        "## Run the model on the Edge TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv2Ingvx8pcI"
      },
      "source": [
        "You can now run the model with acceleration on the Edge TPU.\n",
        "\n",
        "First, download an image of a salad on your device with an Edge TPU. For example, you can use the same one we tested above:\n",
        "\n",
        "```\n",
        "wget https://storage.googleapis.com/cloud-ml-data/img/openimage/3/2520/3916261642_0a504acd60_o.jpg -O salad.jpg\n",
        "```\n",
        "\n",
        "And then make sure you have [installed the PyCoral API](https://coral.ai/software/#pycoral-api).\n",
        "\n",
        "Now run an inference using [this example code for the PyCoral API](https://github.com/google-coral/pycoral/blob/master/examples/detect_image.py). Just clone the GitHub repo and run the example, passing it the model files from this tutorial:\n",
        "\n",
        "```\n",
        "git clone https://github.com/google-coral/pycoral\n",
        "\n",
        "cd pycoral/examples/\n",
        "\n",
        "python3 detect_image.py \\\n",
        "  --model efficientdet-lite-salads_edgetpu.tflite \\\n",
        "  --labels salad-labels.txt \\\n",
        "  --input salad.jpg \\\n",
        "  --output salad_result.jpg\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS4u77W5gnzQ"
      },
      "source": [
        "## More resources\n",
        "\n",
        "* For more information about the Model Maker library used in this tutorial, see the [TensorFlow Lite Model Maker guide](https://www.tensorflow.org/lite/guide/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "\n",
        "* For other transfer learning tutorials that are compatible with the Edge TPU, see the [Colab tutorials for Coral](https://github.com/google-coral/tutorials#colab-tutorials-for-coral).\n",
        "\n",
        "* You can also find more examples that show how to run inference on the Edge TPU at [coral.ai/examples](https://coral.ai/examples/#code-examples/)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "license"
      ],
      "name": "Retrain EfficientDet-Lite detector for the Edge TPU (TF2)",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
